= Redis Flink Connector
:linkattrs:
:name:               Redis Flink Connector
:project-owner:      redis-field-engineering
:project-name:       redis-flink-connector
:project-group:      com.redis
:project-version: 0.0.10
:dist-repo-name:     redis-flink-connector-dist

The Redis Flink Connector is a highly performant, scalable Flink Source and Sink
connector for Redis. It is designed and built to provide a simple, scalable means of
using Redis as a source and Sink for your stream-processing use cases in Flink.

== Partitioned Streams

The Redis Flink Connector supports partitioned streams, allowing you to configure how many
separate partitions you want for your stream of data. This allows you to scale your stream
across a Redis Cluster, allowing Flink to manage the work of coordinating which consumer
owns which stream.

== Exactly-Once Semantics

The Redis Flink Connector supports exactly-once semantics. This is tied into
the checkpointing mechanism in Flink. Please note that "exactly once" refers to
is at the checkpoint level, so in the case of a failure in your pipeline
you may see messages within a checkpoint being delivered more than once

=== Gradle

Add the following to your `build.gradle` file

[source,groovy]
[subs="attributes"]
.build.gradle
----
dependencies {
    implementation '{project-group}:{project-name}-spring:{project-version}'
}
----


== Using the Stream Source

To use the Flink stream source, you can create a `RedisSourceConfig`.

The configuration options are as follows:

The following table describes the fields in that class:

[cols="1,1,1,1",options="header"]
|===
| **Field**            | **Type**            | **Default Value**              | **Required**
| `host`               | `String`            | `"localhost"`                  | No
| `port`               | `int`               | `6379`                         | No
| `password`           | `String`            | `""` (empty string)            | No
| `user`               | `String`            | `"default"`                    | No
| `consumerGroup`      | `String`            | N/A                            | Yes
| `topicName`          | `String`            | N/A                            | Yes
| `numPartitions`      | `int`               | N/A                            | Yes
| `useClusterApi`      | `boolean`           | `false`                        | No
| `requireAck`         | `boolean`           | `true`                         | No
| `startingId`         | `StreamEntryID`     | `StreamEntryID.XGROUP_LAST_ENTRY` | No
|`failedDeserializationStreamName` | `String`            | `""` | No
|`useTls` | `boolean` | `false` | No
|`mtlsParameters` | `MtlsParameters` | `null` | No
|===

You can then initialize the Source Builder using:

[source,java]
----
RedisSourceBuilder<RedisMessage> sourceBuilder = new RedisSourceBuilder<>(sourceConfig, new RedisMessageDeserializer());
----

After that, all that's left is to use your environment to add the source to your pipeline:

[source,java]
----
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.getConfig().setGlobalJobParameters(globalConfig);
env.enableCheckpointing(5000);
env.setParallelism(4);
TypeInformation<RedisMessage> typeInfo = TypeInformation.of(RedisMessage.class);
String sourceName = "Redis to Redis";
env.fromSource(sourceBuilder.build(), WatermarkStrategy.noWatermarks(), sourceName, typeInfo).sinkTo(sinkBuilder.build());
----

== Using the Redis Stream Sink

To use the Redis Stream Sink, you can initialize a `RedisSinkConfig` object with the following:

The following table describes the fields in that class:

[cols="1,1,1,1,1",options="header"]
|===
| **Field**            | **Type**            | **Default Value**              | **Required** | **description**
| `host`               | `String`            | `"localhost"`                  | No           | the Redis host name
| `port`               | `int`               | `6379`                         | No           | the Redis port
| `password`           | `String`            | `""` (empty string)            | No           | the Redis password
| `user`               | `String`            | `"default"`                    | No           | the Redis user
| `topicName`          | `String`            | N/A                            | Yes          | the Topic Name
| `numPartitions`      | `int`               | N/A                            | Yes          | the number of partitions
| `flushOnCheckpoint`  | `boolean`           | `false`                        | No           | whether to flush writes on checkpoint
| `numMessagesToBuffer` | `int`              | `0`                            | No           | number of messages to buffer before flushing (when > 0)
| `flushIntervalMillis` | `int`              | `0`                            | No           | time interval in milliseconds for periodic flushing (when > 0)
| `failedSerializationStreamName` | `String`            | `""` (empty string) | No           | the stream name to serialization errors to
|`useTls` | `boolean` | `false` | No | whether to use TLS
|`mtlsParameters` | `MtlsParameters` | `null` | No | parameters to use for mTLS
|`traceConfiguration` | `SinkTraceConfiguration` | `null` | No | configuration for tracing events processed by the sink
|===

You then have to initialize the builder and sink to it:

[source,java]
----
RedisSinkBuilder<RedisMessage> sinkBuilder = new RedisSinkBuilder<>(new RedisPassthroughSerializer(), sinkConfig);
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.getConfig().setGlobalJobParameters(globalConfig);
env.enableCheckpointing(5000);
env.setParallelism(4);
TypeInformation<RedisMessage> typeInfo = TypeInformation.of(RedisMessage.class);
String sourceName = "Redis to Redis";
env.fromSource(sourceBuilder.build(), WatermarkStrategy.noWatermarks(), sourceName, typeInfo).sinkTo(sinkBuilder.build());
----

== Serializers and Keys

The Redis Flink Connector's natural data type is the `RedisMessage` class. This class contains the data of the message (a `Map<String,String>`) and the key for the Message (a `String`).
The `RedisPasssthroughSerializer` and the `RedisMessageDeserializer` are a simple serializer/deserializer pair that allows you to work directly with the `RedisMessage` object.

The `RedisObjectSerializer` and `RedisObjectDeserializer` are generic serializers/deserializers that allow you to work with your standard serializable POJOs.
You can use these if you want to work with your own objects domain objects, the object is serialized to JSON and added as the
`data` field of the Stream Message that is sent to Redis. If you need to add specific modules to the `ObjectMapper` (e.g. `JavaTimeModule`), you can do so by
passing in an `ObjectMapperSupplier` to the `RedisObjectSerializer` and `RedisObjectDeserializer` constructors. E.g.

[source,java]
----
RedisObjectSerializer<Person> serializer = new RedisObjectSerializer<>(() -> {
            ObjectMapper objectMapper = new ObjectMapper();
            objectMapper.registerModule(new JavaTimeModule());
            return objectMapper;
        });
----

If you use these, you may also want to provide a `RedisKeyExtractor` to extract the key from the object, otherwise, a hashcode extracted from the JSON payload of the object will act as the key.

The key determines what partition that a message will be sent to.

=== Configure Serializer and Key Extractor

You can configure the serializer and key extractor in the `RedisSinkBuilder`:

[source,java]
----
RedisSinkBuilder<Person> sinkBuilder = new RedisSinkBuilder<Person>(new RedisObjectSerializer<>(), sinkConfig).keyExtractor(Person::getName);
----

And you can configure which deserializer to use in the `RedisSourceBuilder`:
[source,java]
----
RedisSourceBuilder<Person> sourceBuilder = new RedisSourceBuilder<>(sourceConfig, new RedisObjectDeserializer<>(Person.class));
----

== Quick Start

You can run the demo in this repo by running:

[source,bash]
----
docker compose up -d
./example-redis-job.sh
----

This will spin up Redis, a Flink Job Manager and Task Manager, and start a Job with Redis as the Source and Sink.

== Sink Trace Configuration

The Redis Sink supports configurable tracing of events processed by the sink. This allows you to track the success or failure of event writes with different levels of detail. Tracing can be configured using the `SinkTraceConfiguration` class with the following options:

=== Trace Levels

The `SinkTraceLevel` enum controls which events are traced:

[cols="1,2",options="header"]
|===
| **Level**   | **Description**
| `NONE`      | No tracing (default when configuration is null)
| `FAILURES`  | Only trace failed writes
| `ALL`       | Trace all events (both successful and failed writes)
|===

=== Trace Contents

The `SinkTraceContents` enum controls what information is included in the trace:

[cols="1,2",options="header"]
|===
| **Contents** | **Description**
| `NONE`       | No content (minimal tracing)
| `METADATA`   | Include metadata about the event (timestamps, keys, etc.)
| `ALL`        | Include full message contents along with metadata
|===

=== Example Configuration

[source,java]
----
// Configure tracing for all events with full message contents
SinkTraceConfiguration traceConfig = new SinkTraceConfiguration(SinkTraceLevel.ALL, SinkTraceContents.ALL);

// Use the configuration when building your sink
RedisSinkConfig config = RedisSinkConfig.builder()
        .host("redis")
        .topicName("output")
        .numPartitions(3)
        .traceConfiguration(traceConfig)
        .build();
----

== Support

{name} is supported by Redis, Inc. for enterprise-tier customers as a 'Developer Tool' under the https://redis.io/legal/software-support-policy/[Redis Software Support Policy.] For non enterprise-tier customers we supply support for {name} on a good-faith basis.
To report bugs, request features, or receive assistance, please https://github.com/{project-owner}/{dist-repo-name}/issues[file an issue].

== License

{name} is licensed under the Business Source License 1.1. Copyright (C) 2024 Redis, Inc. See link:LICENSE.md[LICENSE] for details.